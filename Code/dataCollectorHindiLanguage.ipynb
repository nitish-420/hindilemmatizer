{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re,string"
      ],
      "metadata": {
        "id": "dTsdM2_HY3-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sxdP-HpEZ-yZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35d10525-35b8-45ae-8709-dc95e6bbedf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords=set()\n",
        "\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/stopwords.txt',encoding = 'utf-8') as f:\n",
        "\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    data[i]=data[i].strip()\n",
        "\n",
        "  stopwords=set(data)\n",
        "  \n",
        "  f.close()\n"
      ],
      "metadata": {
        "id": "Yvna0mMOmXsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "triv_tokenizer_indic_pat=re.compile(r'(['+string.punctuation+r'\\u0964\\u0965'+r'])')\n",
        "punct=set(string.punctuation)\n",
        "punct.add('ред')\n",
        "\n",
        "\n",
        "pat_num_seq=re.compile(r'([0-9]+ [,.:/] )+[0-9]+')\n",
        "\n",
        "def tokenize(text): \n",
        "\n",
        "    tok_str=triv_tokenizer_indic_pat.sub(r' \\1 ',text.replace('\\t',' '))\n",
        "    tok_str=re.sub(r'[ ]+',' ',tok_str).strip(' ')\n",
        "    \n",
        "\n",
        "    # s=re.sub(r'[ ]+',' ',tok_str).strip(' ')\n",
        "    \n",
        "    # new_s=''\n",
        "    # prev=0\n",
        "    # for m in pat_num_seq.finditer(s):\n",
        "    #     start=m.start()\n",
        "    #     end=m.end()\n",
        "    #     if start>prev:\n",
        "    #         new_s=new_s+s[prev:start]\n",
        "    #         new_s=new_s+s[start:end].replace(' ','')\n",
        "    #         prev=end\n",
        "   \n",
        "    # new_s=new_s+s[prev:]\n",
        "    # s=new_s\n",
        "    \n",
        "    # return s.split(' ')\n"
      ],
      "metadata": {
        "id": "biHtJZTH4FlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# iitb file traverse\n",
        "def tokenize(sent):\n",
        "  punct_sent=re.sub(r'\\s+', ' ', re.sub(r'[{}[\\]:;?@!#$%^&*()-_\\'\\\"<>\\u0964\\u0965,]', ' ', sent)).split()\n",
        "  return punct_sent\n",
        "\n",
        "words = set()\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/IITBDatabase/data_txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    data[i]=data[i].strip()\n",
        "    for word in tokenize(data[i]):\n",
        "      if word:\n",
        "        words.add(word)\n",
        "  f.close()\n",
        "\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/IITBDatabase/idxnoun_txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    data[i]=data[i].strip()\n",
        "    for word in tokenize(data[i]):\n",
        "      if word:\n",
        "        words.add(word)\n",
        "  f.close()\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/IITBDatabase/data_txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    data[i]=data[i].strip()\n",
        "    for word in tokenize(data[i]):\n",
        "      if word:\n",
        "        words.add(word)\n",
        "  f.close()\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/IITBDatabase/idxadjective_txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    data[i]=data[i].strip()\n",
        "    for word in tokenize(data[i]):\n",
        "      if word:\n",
        "        words.add(word)\n",
        "  f.close()\n",
        "\n",
        "\n",
        "outputFile=open(\"/content/drive/MyDrive/CollabData/Lemmatizer/iitbdataWords.txt\",\"x\")\n",
        "for word in words:\n",
        "  outputFile.write(word+'\\n')\n",
        "outputFile.close()\n",
        "\n",
        "print(len(words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNsH3vh0z7kZ",
        "outputId": "101c54ba-a640-45e8-b2ae-faec44e94880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  For text0.txt file\n",
        "def tokenize(sent):\n",
        "  punct_sent=re.sub(r'\\s+', ' ', re.sub(r'[{}[\\]:;?@!#$%^&*()-_\\'\\\"<>\\u0964\\u0965,]', ' ', sent)).split()\n",
        "  return punct_sent\n",
        "\n",
        "words = set()\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/text0.txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    data[i]=data[i].strip()\n",
        "    for word in tokenize(data[i]):\n",
        "      if word:\n",
        "        words.add(word)\n",
        "\n",
        "outputFile=open(\"/content/drive/MyDrive/CollabData/Lemmatizer/words0.txt\",\"x\")\n",
        "for word in words:\n",
        "  outputFile.write(word+'\\n')\n",
        "outputFile.close()\n",
        "\n",
        "words=list(words)\n",
        "print(len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14dZHMDrEa9X",
        "outputId": "109df425-37f8-41f3-9072-1fd4edc8841e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  For text1.txt file\n",
        "def tokenize(sent):\n",
        "  punct_sent=re.sub(r'\\s+', ' ', re.sub(r'[{}[\\]:;?@!#$%^&*()-_\\'\\\"<>\\u0964\\u0965,]', ' ', sent)).split()\n",
        "  return punct_sent\n",
        "\n",
        "words = set()\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/text1.txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    data[i]=data[i].strip()\n",
        "    for word in tokenize(data[i]):\n",
        "      if word:\n",
        "        words.add(word)\n",
        "\n",
        "outputFile=open(\"/content/drive/MyDrive/CollabData/Lemmatizer/words1.txt\",\"x\")\n",
        "for word in words:\n",
        "  outputFile.write(word+'\\n')\n",
        "outputFile.close()\n",
        "\n",
        "words=list(words)\n",
        "print(len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FNgUhk7VLe6",
        "outputId": "2087672c-6e64-480d-80f7-7558c5518250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://raw.githubusercontent.com/bdrillard/english-hindi-dictionary/master/English-Hindi%20Dictionary.csv for this file data collection named as text2.txt\n",
        "\n",
        "import pandas as pd\n",
        "url = 'https://raw.githubusercontent.com/bdrillard/english-hindi-dictionary/master/English-Hindi%20Dictionary.csv'\n",
        "data=pd.read_csv(url)\n",
        "\n",
        "import re,string\n",
        "words=set()\n",
        "for sent in data['hword']:\n",
        "  punct_sent=re.sub(r'\\s+', ' ', re.sub(r'[{}[\\]:;?@!#$%^&*()-_\\'\\\"<>]', ' ', sent)).split()\n",
        "  for word in punct_sent:\n",
        "    words.add(word)\n",
        "\n",
        "outputFile=open(\"/content/drive/MyDrive/CollabData/Lemmatizer/words2.txt\",\"x\")\n",
        "for word in words:\n",
        "  outputFile.write(word+'\\n')\n",
        "\n",
        "outputFile.close()\n",
        "\n",
        "print(len(words))\n"
      ],
      "metadata": {
        "id": "KtU8pErEzzZf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71580cbc-54ca-49ec-9d51-25facbc3e900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for dataset_wiki file valid.csv\n",
        "\n",
        "import pandas as pd\n",
        "url = '/content/drive/MyDrive/CollabData/Lemmatizer/dataset_wiki/valid.csv'\n",
        "data=pd.read_csv(url)\n",
        "words=set()\n",
        "i=0\n",
        "\n",
        "def is_hindi(text):\n",
        "  hindi_script = re.compile(\"[\\u0900-\\u097F]+\")\n",
        "  return hindi_script.match(text) is not None\n",
        "\n",
        "for sent in data['text']:\n",
        "  if type(sent)!=float:\n",
        "    punct_sent=re.sub(r'\\s+', ' ', re.sub(r'[{}[\\]:;?@!#$%^&*()-_\\'\\\"<>\\u0964\\u0965,]', ' ', sent.encode(\"utf-8\").decode(\"utf-8\"))).split()\n",
        "    for word in punct_sent:\n",
        "      if is_hindi(word):\n",
        "        words.add(word)\n",
        "  if i==34455:\n",
        "    break\n",
        "  i+=1\n",
        "\n",
        "outputFile=open(\"/content/drive/MyDrive/CollabData/Lemmatizer/dataset_wiki/words_valid.txt\",\"x\")\n",
        "for word in words:\n",
        "  outputFile.write(word+'\\n')\n",
        "\n",
        "outputFile.close()\n",
        "print(len(words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPg9_Sk-9R1b",
        "outputId": "42207d38-1f75-4955-e6ea-06070b67386c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "269584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for dataset_wiki file train.csv\n",
        "\n",
        "import pandas as pd\n",
        "url = '/content/drive/MyDrive/CollabData/Lemmatizer/dataset_wiki/train.csv'\n",
        "data=pd.read_csv(url)\n",
        "words=set()\n",
        "i=0\n",
        "\n",
        "def is_hindi(text):\n",
        "  hindi_script = re.compile(\"[\\u0900-\\u097F]+\")\n",
        "  return hindi_script.match(text) is not None\n",
        "\n",
        "for sent in data['text']:\n",
        "  if type(sent)!=float:\n",
        "    punct_sent=re.sub(r'\\s+', ' ', re.sub(r'[{}[\\]:;?@!#$%^&*()-_\\'\\\"<>\\u0964\\u0965,]', ' ', sent.encode(\"utf-8\").decode(\"utf-8\"))).split()\n",
        "    for word in punct_sent:\n",
        "      if is_hindi(word):\n",
        "        words.add(word)\n",
        "  if i==137823:\n",
        "    break\n",
        "  i+=1\n",
        "\n",
        "outputFile=open(\"/content/drive/MyDrive/CollabData/Lemmatizer/dataset_wiki/words_train.txt\",\"x\")\n",
        "for word in words:\n",
        "  outputFile.write(word+'\\n')\n",
        "\n",
        "outputFile.close()\n",
        "print(len(words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzIxMvn2GWlH",
        "outputId": "9123b55d-1b15-40ea-e23b-3b40af3b6add"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "531355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for file goodwords0.txt which have words0-wrong0+add0 as sets\n",
        "finalWords0=set()\n",
        "\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/words0.txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    finalWords0.add(data[i].strip())\n",
        "  f.close()\n",
        "\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/wrong0.txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    finalWords0.discard(data[i].strip())\n",
        "  f.close()\n",
        "\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/add0.txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    finalWords0.add(data[i].strip())\n",
        "  f.close()\n",
        "\n",
        "outputFile=open(\"/content/drive/MyDrive/CollabData/Lemmatizer/goodwords0.txt\",\"x\")\n",
        "for word in finalWords0:\n",
        "  outputFile.write(word+'\\n')\n",
        "\n",
        "outputFile.close()\n",
        "\n",
        "print(len(finalWords0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnvTg2QDeKjh",
        "outputId": "5d1ab08c-4328-4451-9ea8-b75e0c8e93b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for file goodwords1.txt which have words1-wrong1+add1 as sets\n",
        "finalWords1=set()\n",
        "\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/words1.txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    finalWords1.add(data[i].strip())\n",
        "  f.close()\n",
        "\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/wrong1.txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    finalWords1.discard(data[i].strip())\n",
        "  f.close()\n",
        "\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/add1.txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    finalWords1.add(data[i].strip())\n",
        "\n",
        "outputFile=open(\"/content/drive/MyDrive/CollabData/Lemmatizer/goodwords1.txt\",\"x\")\n",
        "for word in finalWords1:\n",
        "  outputFile.write(word+'\\n')\n",
        "\n",
        "outputFile.close()\n",
        "print(len(finalWords1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm_5VMq5hjSa",
        "outputId": "5df2dac7-9f16-4ab0-8158-929b62dbee2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for file goodwords2.txt which have words2-wrong2+add2 as sets\n",
        "finalWords2=set()\n",
        "\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/words2.txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    finalWords2.add(data[i].strip())\n",
        "  f.close()\n",
        "\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/wrong2.txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    finalWords2.discard(data[i].strip())\n",
        "  f.close()\n",
        "\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/add2.txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    finalWords2.add(data[i].strip())\n",
        "  f.close()\n",
        "\n",
        "outputFile=open(\"/content/drive/MyDrive/CollabData/Lemmatizer/goodwords2.txt\",\"x\")\n",
        "for word in finalWords2:\n",
        "  outputFile.write(word+'\\n')\n",
        "\n",
        "outputFile.close()\n",
        "print(len(finalWords2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykTjXmbwhvXv",
        "outputId": "0b8b206d-d6ba-43fa-a93d-a21a54716006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a file with all goodwords set\n",
        "finalWords = set()\n",
        "\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/goodwords0.txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    finalWords.add(data[i].strip())\n",
        "  f.close()\n",
        "\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/goodwords1.txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    finalWords.add(data[i].strip())\n",
        "  f.close()\n",
        "\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/goodwords2.txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    finalWords.add(data[i].strip())\n",
        "  f.close()\n",
        "\n",
        "outputFile=open(\"/content/drive/MyDrive/CollabData/Lemmatizer/allGoodWords.txt\",\"x\")\n",
        "for word in finalWords:\n",
        "  outputFile.write(word+'\\n')\n",
        "\n",
        "outputFile.close()\n",
        "\n",
        "print(len(finalWords))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX2K1UdgjB5_",
        "outputId": "af84fda7-8767-40cd-d55b-e54f8d47d039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for getting words from the inltk files\n",
        "finalWords = set()\n",
        "\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/inltkText0.txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    if data[i] and data[i][0]=='_' and len(data[i]>3):\n",
        "      finalWords.add(data[i][1:].strip())\n",
        "\n",
        "  f.close()\n",
        "\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/inltkText1FileSample.txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    if data[i] and len(data[i])>2:\n",
        "      finalWords.add(data[i].strip())\n",
        "  f.close()\n",
        "\n",
        "outputFile=open(\"/content/drive/MyDrive/CollabData/Lemmatizer/inltkWords.txt\",\"x\")\n",
        "for word in finalWords:\n",
        "  outputFile.write(word+'\\n')\n",
        "\n",
        "outputFile.close()\n",
        "\n",
        "print(len(finalWords))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rm1FFU1RW1S2",
        "outputId": "64b5be7f-1604-4bc4-d5ed-8e49b1b986ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for making a big set of all the words collected by us so far\n",
        "finalWords = set()\n",
        "\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/goodwords0.txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    finalWords.add(data[i].strip())\n",
        "  f.close()\n",
        "\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/goodwords1.txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    finalWords.add(data[i].strip())\n",
        "  f.close()\n",
        "\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/goodwords2.txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    finalWords.add(data[i].strip())\n",
        "  f.close()\n",
        "\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/dataset_wiki/words_train.txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    finalWords.add(data[i].strip())\n",
        "  f.close()\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/dataset_wiki/words_valid.txt',encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    finalWords.add(data[i].strip())\n",
        "  f.close()\n",
        "with open(\"/content/drive/MyDrive/CollabData/Lemmatizer/inltkWords.txt\",encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    finalWords.add(data[i].strip())\n",
        "  f.close()\n",
        "with open(\"/content/drive/MyDrive/CollabData/Lemmatizer/githubDatasetWords.txt\",encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    finalWords.add(data[i].strip())\n",
        "  f.close()\n",
        "with open(\"/content/drive/MyDrive/CollabData/Lemmatizer/iitbdataWords.txt\",encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    finalWords.add(data[i].strip())\n",
        "  f.close()\n",
        "with open(\"/content/drive/MyDrive/CollabData/Lemmatizer/universalDependencyWords.txt\",encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    finalWords.add(data[i].strip())\n",
        "  f.close()\n",
        "# outputFile=open(\"/content/drive/MyDrive/CollabData/Lemmatizer/FinalData/totalWords.txt\",\"x\")\n",
        "# for word in finalWords:\n",
        "#   outputFile.write(word+'\\n')\n",
        "\n",
        "# outputFile.close()\n",
        "print(len(set(finalWords)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_azwf01r97hd",
        "outputId": "00137209-6c23-4775-b8d1-71eb40626abf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "647731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# total rules/exceptions files into direct.csv\n",
        "import csv\n",
        "csvData = []\n",
        "def eval(root,word):\n",
        "  if root[0]==word[0] and len(word)>len(root):\n",
        "    csvData.append((word,root))\n",
        "    \n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/FinalRootWords/ruleBased1.csv', encoding='utf-8') as f:\n",
        "  reader = csv.reader(f)\n",
        "  for row in reader:\n",
        "    word,pre, root, suff = row\n",
        "    eval(root,word)\n",
        "  f.close()\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/FinalRootWords/addons0.csv', encoding='utf-8') as f:\n",
        "  reader = csv.reader(f)\n",
        "  for row in reader:\n",
        "    word, root = row\n",
        "    eval(root,word)\n",
        "  f.close()\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/FinalRootWords/addons1.csv', encoding='utf-8') as f:\n",
        "  reader = csv.reader(f)\n",
        "  for row in reader:\n",
        "    word, root = row\n",
        "    eval(root,word)\n",
        "  f.close()\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/FinalRootWords/addons2.csv', encoding='utf-8') as f:\n",
        "  reader = csv.reader(f)\n",
        "  for row in reader:\n",
        "    word, root = row\n",
        "    eval(root,word)\n",
        "  f.close()\n",
        "# with open('/content/drive/MyDrive/CollabData/Lemmatizer/FinalRootWords/addons3.csv', encoding='utf-8') as f:\n",
        "#   reader = csv.reader(f)\n",
        "#   for row in reader:\n",
        "#     word, root = row\n",
        "#     eval(root,word)\n",
        "#   f.close()\n",
        "\n",
        "with open(f'/content/drive/MyDrive/CollabData/Lemmatizer/FinalData/direct.csv', 'w', newline='') as f:\n",
        "  writer=csv.writer(f)\n",
        "  for row in csvData:\n",
        "    writer.writerow(row)\n",
        "  f.close()\n",
        "print(len(csvData))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeVG3HTkNmc1",
        "outputId": "09825f40-07e4-4639-dfc5-2b78f69a7d13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install conllu\n",
        "!pip install requests\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmnv53yFcVE2",
        "outputId": "b795f22b-434e-4af9-b3ae-330301e5978c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.10/dist-packages (4.5.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/mahimagupta/NLP-Research-Project-/master/datasets/hi_hdtb-ud-dev.conllu\"\n",
        "file_path = \"hi_hdtb-ud-dev.conllu\"\n",
        "\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    with open(\"/content/drive/MyDrive/CollabData/Lemmatizer/datasetGithub.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(response.text)\n",
        "    print(\"File downloaded successfully.\")\n",
        "else:\n",
        "    print(\"Failed to download the file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAhXQ2TleUNx",
        "outputId": "b6650749-5be2-4ad0-cb12-efd6e9fcdd1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# word and root for addons from given link https://raw.githubusercontent.com/mahimagupta/NLP-Research-Project-/master/datasets/hi_hdtb-ud-dev.conllu\n",
        "\n",
        "csvData =set()\n",
        "samewords = []\n",
        "with open(\"/content/drive/MyDrive/CollabData/Lemmatizer/datasetGithub.txt\",encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for sent in data:\n",
        "    arr= sent.split()\n",
        "    try:\n",
        "      if arr[0] and arr[1] and arr[2] and arr[0].isnumeric():\n",
        "        if arr[1]==arr[2] and arr[1]!=',':\n",
        "          samewords.append(arr[1])\n",
        "          csvData.add((arr[1],arr[2]))\n",
        "        elif arr[1]!=',':\n",
        "          csvData.add((arr[1],arr[2]))\n",
        "          samewords.append(arr[1], arr[2])\n",
        "    except:\n",
        "      continue\n",
        "  f.close()\n",
        "\n",
        "import csv\n",
        "\n",
        "with open(f'/content/drive/MyDrive/CollabData/Lemmatizer/FinalRootWords/addons1.csv', 'w', newline='') as f:\n",
        "  writer=csv.writer(f)\n",
        "  for row in csvData:\n",
        "    writer.writerow(row)\n",
        "  f.close()\n",
        "  \n",
        "# finalData = []\n",
        "# for word in samewords: \n",
        "#   if len(word)>1:\n",
        "#     finalData.append(word)\n",
        "\n",
        "# outputFile=open(\"/content/drive/MyDrive/CollabData/Lemmatizer/githubDatasetWords.txt\",\"x\")\n",
        "# for word in samewords:\n",
        "#   outputFile.write(word+'\\n')\n",
        "# outputFile.close()\n",
        "\n",
        "# print(len(finalData))\n",
        "print(len(csvData))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBlE8rROcDFI",
        "outputId": "da138c6b-e14b-46bb-a220-70785ac3d3a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is to collect proper nouns from universal dataset to increase the accuracy of lemmatizer\n"
      ],
      "metadata": {
        "id": "Vzx4rZtVcDTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# requests for other two conllu files from UniversalDependencies https://github.com/UniversalDependencies/UD_Hindi-HDTB\n",
        "url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_Hindi-HDTB/master/hi_hdtb-ud-test.conllu\"\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    with open(\"/content/drive/MyDrive/CollabData/Lemmatizer/datasetUniversalDep1.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(response.text)\n",
        "    print(\"File downloaded successfully.\")\n",
        "else:\n",
        "    print(\"Failed to download the file.\")\n",
        "\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_Hindi-HDTB/master/hi_hdtb-ud-train.conllu\"\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    with open(\"/content/drive/MyDrive/CollabData/Lemmatizer/datasetUniversalDep2.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(response.text)\n",
        "    print(\"File downloaded successfully.\")\n",
        "else:\n",
        "    print(\"Failed to download the file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9uA5nwrIqYI",
        "outputId": "3d5d6b4b-ef06-43d0-b039-f0b3967a42ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully.\n",
            "File downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# word and root for addons for above data\n",
        "\n",
        "csvData =set()\n",
        "samewords = set()\n",
        "with open(\"/content/drive/MyDrive/CollabData/Lemmatizer/datasetUniversalDep1.txt\",encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for sent in data:\n",
        "    arr= sent.split()\n",
        "    try:\n",
        "      if arr[0] and arr[1] and arr[2] and arr[0].isnumeric():\n",
        "        if arr[1]==arr[2] and arr[1]!=',':\n",
        "          samewords.add(arr[1])\n",
        "          csvData.add((arr[1],arr[2]))\n",
        "        elif arr[1]!=',':\n",
        "          csvData.add((arr[1],arr[2]))\n",
        "          samewords.add(arr[1], arr[2])\n",
        "    except:\n",
        "      continue\n",
        "  f.close()\n",
        "\n",
        "with open(\"/content/drive/MyDrive/CollabData/Lemmatizer/datasetUniversalDep2.txt\",encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for sent in data:\n",
        "    arr= sent.split()\n",
        "    try:\n",
        "      if arr[0] and arr[1] and arr[2] and arr[0].isnumeric():\n",
        "        if arr[1]==arr[2] and arr[1]!=',':\n",
        "          samewords.add(arr[1])\n",
        "          csvData.add((arr[1],arr[2]))\n",
        "        elif arr[1]!=',':\n",
        "          csvData.add((arr[1],arr[2]))\n",
        "          samewords.add(arr[1], arr[2])\n",
        "    except:\n",
        "      continue\n",
        "  f.close()\n",
        "\n",
        "import csv\n",
        "\n",
        "with open(f'/content/drive/MyDrive/CollabData/Lemmatizer/FinalRootWords/addons2.csv', 'w', newline='') as f:\n",
        "  writer=csv.writer(f)\n",
        "  for row in csvData:\n",
        "    writer.writerow(row)\n",
        "  f.close()\n",
        "  \n",
        "# finalData = []\n",
        "# for word in samewords: \n",
        "#   if len(word)>1:\n",
        "#     finalData.append(word)\n",
        "\n",
        "# outputFile=open(\"/content/drive/MyDrive/CollabData/Lemmatizer/universalDependencyWords.txt\",\"x\")\n",
        "# for word in finalData:\n",
        "#   outputFile.write(word+'\\n')\n",
        "# outputFile.close()\n",
        "\n",
        "# print(len(finalData))\n",
        "print(len(csvData))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmm_T0usKnvu",
        "outputId": "95101c4d-34f4-43b0-dfc6-4072e119dd62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rules from https://raw.githubusercontent.com/mahimagupta/NLP-Research-Project-/master/Final%20Implementation/rules.txt this repo\n",
        "csvData =set()\n",
        "samewords = set()\n",
        "with open(\"/content/drive/MyDrive/CollabData/Lemmatizer/FinalRootWords/rules.txt\",encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for sent in data:\n",
        "    arr= sent.split()\n",
        "    csvData.add((arr[0],arr[2]))\n",
        "  f.close()\n",
        "\n",
        "with open(\"/content/drive/MyDrive/CollabData/Lemmatizer/FinalRootWords/rules2.txt\",encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for sent in data:\n",
        "    arr= sent.split()\n",
        "    csvData.add((arr[0],arr[3]))\n",
        "  f.close()\n",
        "\n",
        "import csv\n",
        "\n",
        "with open(f'/content/drive/MyDrive/CollabData/Lemmatizer/FinalRootWords/addons3.csv', 'w', newline='') as f:\n",
        "  writer=csv.writer(f)\n",
        "  for row in csvData:\n",
        "    writer.writerow(row)\n",
        "  f.close()\n",
        "print(len(csvData))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFPDH67TQeSy",
        "outputId": "cb371b79-a5ce-453d-a1bb-4796b2dc402c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# making 13 csv files for all datas \n",
        "import csv\n",
        "csvData = []\n",
        "with open(\"/content/drive/MyDrive/CollabData/Lemmatizer/allGoodWords.txt\",encoding = 'utf-8') as f:\n",
        "  data=f.readlines()\n",
        "  for i in range(len(data)):\n",
        "    word=data[i].strip()\n",
        "    rowData=\", \".join([f'( {i} - {word[i]} )' for i in range(len(word))])\n",
        "    csvData.append([word,rowData,\"0\",str(len(word))])\n",
        "  f.close()\n",
        "\n",
        "# with open('/content/drive/MyDrive/CollabData/Lemmatizer/allGoodWordsStartEnd.csv', 'w', newline='') as f:\n",
        "#   writer = csv.writer(f)\n",
        "#   for row in csvData:\n",
        "#     writer.writerow(row)\n",
        "#   f.close()\n",
        "\n",
        "# here breaking it into 13 files of approx 5k values\n",
        "dataIndex = 0\n",
        "fileIndex = 0\n",
        "while True:\n",
        "  z=0\n",
        "  with open(f'/content/drive/MyDrive/CollabData/Lemmatizer/RootWords/file{fileIndex}.csv', 'w', newline='') as f:\n",
        "    writer=csv.writer(f)\n",
        "    for _ in range(5000):\n",
        "      if dataIndex>=len(csvData):\n",
        "        z=1\n",
        "        break\n",
        "      writer.writerow(csvData[dataIndex])\n",
        "      dataIndex+=1\n",
        "    f.close()\n",
        "  if z:\n",
        "    break\n",
        "  fileIndex+=1\n",
        "    \n"
      ],
      "metadata": {
        "id": "P5sOQq0Ty8XY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "a2f96992-d97a-401d-fb4f-a010fc25cb9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b33d545dabeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcsvData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/CollabData/Lemmatizer/allGoodWords.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/CollabData/Lemmatizer/allGoodWords.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is code to convert root words indexes file made by amit and paridhi to dictionary format we need for the algorithm\n",
        "import csv \n",
        "csvData = []\n",
        "\n",
        "def sanitizeWord(word, type):\n",
        "  \n",
        "  '''\n",
        "    type\n",
        "    0 - prefix\n",
        "    1 - root\n",
        "    2 - suffix\n",
        "  '''\n",
        "\n",
        "  if not word:\n",
        "    return word\n",
        "  word=list(word)\n",
        "  if word[0]=='реБ':\n",
        "    word[0]='рдЙ'\n",
        "  return \"\".join(word)\n",
        "\n",
        "def sanitizeRow(word,left,right):\n",
        "  return [word, sanitizeWord(word[:left],0),sanitizeWord(word[left:right],1), sanitizeWord(word[right:],2)]\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/FinalRootWords/file2.csv', encoding='utf-8') as f:\n",
        "  reader = csv.reader(f)\n",
        "  index = 0\n",
        "  for row in reader:\n",
        "    index+=1\n",
        "    if index==3000:\n",
        "      break\n",
        "    word,ignore,left,right = row\n",
        "    try:\n",
        "      left=int(left)\n",
        "      right=int(right)\n",
        "    except: \n",
        "      continue\n",
        "    if left==-1 or right==-1:\n",
        "      continue\n",
        "    csvData.append(sanitizeRow(word,left,right))\n",
        "  f.close()\n",
        "\n",
        "with open(f'/content/drive/MyDrive/CollabData/Lemmatizer/FinalRootWords/data2.csv', 'w', newline='') as f:\n",
        "  writer=csv.writer(f)\n",
        "  for row in csvData:\n",
        "    writer.writerow(row)\n",
        "  f.close()\n"
      ],
      "metadata": {
        "id": "n2x-qqwS9Rln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is to convert final1.csv to data1.csv which will be part of final dataset that will be sended to python pip library\n",
        "\n",
        "import csv \n",
        "csvData = []\n",
        "\n",
        "def sanitizeWord(word, type):\n",
        "  \n",
        "  '''\n",
        "    type\n",
        "    0 - prefix\n",
        "    1 - root\n",
        "    2 - suffix\n",
        "  '''\n",
        "\n",
        "  if not word:\n",
        "    return word\n",
        "  word=list(word)\n",
        "  if word[0]=='реБ':\n",
        "    word[0]='рдЙ'\n",
        "  return \"\".join(word)\n",
        "\n",
        "def sanitizeRow(word,left,right,extra):\n",
        "  return [word, sanitizeWord(word[:left],0),sanitizeWord(word[left:right]+extra,1), sanitizeWord(word[right:],2)]\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/FinalRootWords/final1.csv', encoding='utf-8') as f:\n",
        "  reader = csv.reader(f)\n",
        "  for row in reader:\n",
        "    word,ignore,left,right, extra = row\n",
        "    try:\n",
        "      left=int(left)\n",
        "      right=int(right)\n",
        "    except:\n",
        "      continue\n",
        "    if left==-1 or right==-1:\n",
        "      continue\n",
        "    csvData.append(sanitizeRow(word,left,right,extra))\n",
        "  f.close()\n",
        "\n",
        "with open(f'/content/drive/MyDrive/CollabData/Lemmatizer/FinalRootWords/data1.csv', 'w', newline='') as f:\n",
        "  writer=csv.writer(f)\n",
        "  for row in csvData:\n",
        "    writer.writerow(row)\n",
        "  f.close()\n"
      ],
      "metadata": {
        "id": "DBlRK-QIp8qG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is to collect examples of rule based approach from final1.csv file\n",
        "\n",
        "import csv \n",
        "csvData = []\n",
        "\n",
        "def sanitizeWord(word, type):\n",
        "  \n",
        "  '''\n",
        "    type\n",
        "    0 - prefix\n",
        "    1 - root\n",
        "    2 - suffix\n",
        "  '''\n",
        "\n",
        "  if not word:\n",
        "    return word\n",
        "  word=list(word)\n",
        "  if word[0]=='реБ':\n",
        "    word[0]='рдЙ'\n",
        "  return \"\".join(word)\n",
        "\n",
        "def sanitizeRow(word,left,right,extra):\n",
        "  return [word, sanitizeWord(word[:left],0),sanitizeWord(word[left:right]+extra,1), sanitizeWord(word[right:],2)]\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/FinalRootWords/final1.csv', encoding='utf-8') as f:\n",
        "  reader = csv.reader(f)\n",
        "  for row in reader:\n",
        "    word,ignore,left,right, extra = row\n",
        "    try:\n",
        "      left=int(left)\n",
        "      right=int(right)\n",
        "    except:\n",
        "      continue\n",
        "    if left==-1 or right==-1 or not extra:\n",
        "      continue\n",
        "    csvData.append(sanitizeRow(word,left,right,extra))\n",
        "  f.close()\n",
        "\n",
        "with open(f'/content/drive/MyDrive/CollabData/Lemmatizer/FinalRootWords/ruleBased1.csv', 'w', newline='') as f:\n",
        "  writer=csv.writer(f)\n",
        "  for row in csvData:\n",
        "    writer.writerow(row)\n",
        "  f.close()\n"
      ],
      "metadata": {
        "id": "9PUDBZSDtYGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# code to collect data0.csv, data1.csv, data2.csv in data.csv all in one file for package\n",
        "csvData = []\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/FinalRootWords/data0.csv', encoding='utf-8') as f:\n",
        "  reader = csv.reader(f)\n",
        "  for row in reader:\n",
        "    word,pre, root, suff = row\n",
        "    csvData.append((word,pre,root,suff))\n",
        "  f.close()\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/FinalRootWords/data1.csv', encoding='utf-8') as f:\n",
        "  reader = csv.reader(f)\n",
        "  for row in reader:\n",
        "    word,pre, root, suff = row\n",
        "    csvData.append((word,pre,root,suff))\n",
        "  f.close()\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/FinalRootWords/data2.csv', encoding='utf-8') as f:\n",
        "  reader = csv.reader(f)\n",
        "  for row in reader:\n",
        "    word,pre, root, suff = row\n",
        "    csvData.append((word,pre,root,suff))\n",
        "  f.close()\n",
        "with open(f'/content/drive/MyDrive/CollabData/Lemmatizer/FinalData/data.csv', 'w', newline='') as f:\n",
        "  writer=csv.writer(f)\n",
        "  for row in csvData:\n",
        "    writer.writerow(row)\n",
        "  f.close()\n",
        "print(len(csvData))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5psjIleJ7VvR",
        "outputId": "adb80195-f2ce-4ad3-c770-1ecf2cea717c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# % of inflected words have suffix prefix and both\n",
        "\n",
        "inflected = 0\n",
        "both = 0\n",
        "suf = 0\n",
        "pre=0\n",
        "with open('/content/drive/MyDrive/CollabData/Lemmatizer/FinalData/data.csv', encoding='utf-8') as f:\n",
        "  reader = csv.reader(f)\n",
        "  for row in reader:\n",
        "    word,pree, root, suff = row\n",
        "    if pree or suff:\n",
        "      inflected +=1\n",
        "    if pree and suff:\n",
        "      both+=1\n",
        "    elif pree:\n",
        "      pre+=1\n",
        "    elif suff: \n",
        "      suf+=1\n",
        "  f.close()\n",
        "\n",
        "print(both/inflected*100)\n",
        "print(pre/inflected*100)\n",
        "print(suf/inflected*100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSm1OotSEt3U",
        "outputId": "882492cf-4a83-4a19-dfb0-2ae084abf2c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5402365716501842\n",
            "15.16385495443087\n",
            "82.29590847391894\n"
          ]
        }
      ]
    }
  ]
}